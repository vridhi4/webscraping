{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8626ef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.10/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.10/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.10/site-packages (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.10/site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./anaconda3/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.10/site-packages (from requests) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0de22b",
   "metadata": {},
   "source": [
    "# part 1 -  scrape atleast 20 pages of product listing pages Items to scrape Product URL , Product Name, Product Price ,Rating, Number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d4eb8058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to finalproducts.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "    product_data = []  # List to store product data\n",
    "\n",
    "    for product in products:\n",
    "        product_url_element = product.find('a', {'class': 'a-link-normal s-no-outline'})\n",
    "        product_name_element = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "        product_price_element = product.find('span', {'class': 'a-offscreen'})\n",
    "        product_rating_element = product.find('span', {'class': 'a-icon-alt'})\n",
    "        product_reviews_element = product.find('span', {'class': 'a-size-base'})\n",
    "\n",
    "        # Check if the elements exist before accessing their text attribute\n",
    "        if product_url_element:\n",
    "            product_url = \"amazon.in\" + product_url_element['href']\n",
    "        else:\n",
    "            product_url = \"Not available\"\n",
    "\n",
    "        if product_name_element:\n",
    "            product_name = product_name_element.text\n",
    "        else:\n",
    "            product_name = \"Not available\"\n",
    "\n",
    "        if product_price_element:\n",
    "            product_price = product_price_element.text\n",
    "        else:\n",
    "            product_price = \"Not available\"\n",
    "\n",
    "        if product_rating_element:\n",
    "            product_rating = product_rating_element.text\n",
    "        else:\n",
    "            product_rating = \"Not available\"\n",
    "\n",
    "        if product_reviews_element:\n",
    "            product_reviews = product_reviews_element.text\n",
    "        else:\n",
    "            product_reviews = \"Not available\"\n",
    "\n",
    "        # Append the product data to the list\n",
    "        product_data.append([product_url, product_name, product_price, product_rating, product_reviews])\n",
    "\n",
    "    return product_data\n",
    "\n",
    "base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_\"\n",
    "num_pages = 20\n",
    "\n",
    "all_product_data = []\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    page_url = base_url + str(page)\n",
    "    product_data = scrape_page(page_url)\n",
    "    all_product_data.extend(product_data)\n",
    "\n",
    "# Save the product data to a CSV file\n",
    "csv_file = 'finalproducts.csv'\n",
    "header = ['URL', 'Name', 'Price', 'Rating', 'Reviews']\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(all_product_data)\n",
    "\n",
    "print(\"Data saved to\", csv_file)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba72774",
   "metadata": {},
   "source": [
    "# part 2 - With the Product URL received in the above case, hit each URL, and add below items: Description, ASIN, Product Description, Manufacturer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome WebDriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode, without opening a browser window\n",
    "service = Service(\"/Users/vridhikumar/Desktop/chromedriver_mac64\\ 2/chromedriver\")  # Replace with the path to your chromedriver executable\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Title', 'Description', 'ASIN', 'Manufacturer', 'URL'])\n",
    "\n",
    "# Function to scrape the product page\n",
    "def scrape_product_page(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source after JavaScript execution\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Create BeautifulSoup object for parsing\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Extract the title from the product page\n",
    "    title_element = soup.find('span', {'id': 'productTitle'})\n",
    "    title = title_element.get_text(strip=True) if title_element else None\n",
    "\n",
    "    # Extract the description from the product page\n",
    "    description_elem = soup.find('div', {'id': 'productDescription'})\n",
    "    description = description_elem.get_text(strip=True) if description_elem else 'N/A'\n",
    "    \n",
    "    # Extract the ASIN and Manufacturer from the product page\n",
    "    asin = None\n",
    "    manufacturer = None\n",
    "    div_element = soup.find('div', {'id': 'detailBullets_feature_div'})\n",
    "    if div_element:\n",
    "        li_elements = div_element.find_all('li')\n",
    "        if len(li_elements) > 3:\n",
    "            asin_element = li_elements[3].find('span', class_=False)\n",
    "            asin = asin_element.text if asin_element else None\n",
    "            manufacturer_element = li_elements[2].find('span', class_=False)\n",
    "            manufacturer = manufacturer_element.text if manufacturer_element else None\n",
    "\n",
    "    # Print or store the extracted information\n",
    "    row = {'Title': title, 'Description': description, 'ASIN': asin, 'Manufacturer': manufacturer, 'URL': url}\n",
    "    df.loc[len(df)] = row\n",
    "    \n",
    "    print(\"Title:\", title)\n",
    "    print(\"Description:\", description)\n",
    "    print(\"ASIN:\", asin)\n",
    "    print(\"Manufacturer:\", manufacturer)\n",
    "    print(\"URL:\", url)\n",
    "    print()\n",
    "\n",
    "# URL of the search page\n",
    "search_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "\n",
    "# Send a GET request to the search page and retrieve the HTML content\n",
    "response = requests.get(search_url)\n",
    "html_content = response.text\n",
    "\n",
    "# Create BeautifulSoup object for parsing the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all the product elements on the page\n",
    "product_elements = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "# Extract the product URLs\n",
    "product_urls = []\n",
    "for element in product_elements:\n",
    "    link_element = element.find('a', {'class': 'a-link-normal s-no-outline'})\n",
    "    if link_element:\n",
    "        product_url = \"https://www.amazon.in\" + link_element['href']\n",
    "        product_urls.append(product_url)\n",
    "\n",
    "# Scrape each product page\n",
    "for url in product_urls:\n",
    "    scrape_product_page(url)\n",
    "\n",
    "# Close the browser and WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(df)\n",
    "\n",
    "# Save DataFrame as CSV file\n",
    "df.to_csv('product_details.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1aa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44a5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d5051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf827b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a94ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba5568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfe724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb291e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
